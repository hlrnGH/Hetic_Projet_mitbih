{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tous les modèles"
      ],
      "metadata": {
        "id": "GgTKpjWLlNXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modèle de base (LSTM)"
      ],
      "metadata": {
        "id": "OUba02KVlSU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(187, 1), return_sequences=False),\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "B9MW7WqQlSCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "def create_model2():\n",
        "    model = Sequential([\n",
        "        LSTM(64, input_shape=(187, 1), return_sequences=False),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "dOD2Nw2GljgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle\n",
        "def create_model3():\n",
        "    model = Sequential([\n",
        "        LSTM(128, input_shape=(187, 1), return_sequences=True),\n",
        "        LSTM(64),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XO6YMp2Alu1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction de la data augmentation et deux modèle"
      ],
      "metadata": {
        "id": "-JoDzD_wlbEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(X_data, y_data, target_class, augmentation_factor):\n",
        "    \"\"\"Augmentation améliorée des données\"\"\"\n",
        "    class_indices = np.where(y_data == target_class)[0]\n",
        "    X_class = X_data[class_indices]\n",
        "    augmented_X = []\n",
        "    augmented_y = []\n",
        "\n",
        "    for signal in X_class:\n",
        "        for _ in range(augmentation_factor):\n",
        "            # Combinaison de plusieurs techniques\n",
        "            augmented_signal = signal.copy()\n",
        "\n",
        "            # Bruit gaussien avec amplitude variable\n",
        "            noise_level = np.random.uniform(0.01, 0.03)\n",
        "            augmented_signal += np.random.normal(0, noise_level, signal.shape)\n",
        "\n",
        "            # Décalage temporel aléatoire\n",
        "            shift = np.random.randint(-10, 10)\n",
        "            augmented_signal = np.roll(augmented_signal, shift, axis=0)\n",
        "\n",
        "            # Scaling amplitude\n",
        "            scale = np.random.uniform(0.9, 1.1)\n",
        "            augmented_signal *= scale\n",
        "\n",
        "            # Inversion temporelle aléatoire\n",
        "            if np.random.random() > 0.5:\n",
        "                augmented_signal = augmented_signal[::-1]\n",
        "\n",
        "            augmented_X.append(augmented_signal)\n",
        "            augmented_y.append(target_class)\n",
        "\n",
        "    return np.array(augmented_X), np.array(augmented_y)\n",
        "\n",
        "def balance_classes(X_data, y_data):\n",
        "    \"\"\"Version améliorée de l'équilibrage des classes\"\"\"\n",
        "    class_counts = np.bincount(y_data)\n",
        "    target_count = np.max(class_counts)\n",
        "\n",
        "    # Stockage des données augmentées\n",
        "    all_augmented_X = []\n",
        "    all_augmented_y = []\n",
        "\n",
        "    for class_label in range(len(class_counts)):\n",
        "        if class_counts[class_label] < target_count:\n",
        "            needed = target_count - class_counts[class_label]\n",
        "\n",
        "            # Calculer le facteur d'augmentation\n",
        "            base_count = class_counts[class_label]\n",
        "            augmentation_factor = int(np.ceil(needed / base_count))\n",
        "\n",
        "            # Générer plus de données que nécessaire\n",
        "            X_aug, y_aug = augment_data(\n",
        "                X_data, y_data,\n",
        "                class_label, augmentation_factor\n",
        "            )\n",
        "\n",
        "            # Sélectionner le nombre exact nécessaire\n",
        "            indices = np.random.choice(len(X_aug), needed, replace=False)\n",
        "            all_augmented_X.append(X_aug[indices])\n",
        "            all_augmented_y.append(y_aug[indices])\n",
        "\n",
        "    if len(all_augmented_X) > 0:\n",
        "        X_balanced = np.concatenate([X_data] + all_augmented_X)\n",
        "        y_balanced = np.concatenate([y_data] + all_augmented_y)\n",
        "\n",
        "        # Mélanger les données\n",
        "        indices = np.random.permutation(len(X_balanced))\n",
        "        X_balanced = X_balanced[indices]\n",
        "        y_balanced = y_balanced[indices]\n",
        "\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "    return X_data, y_data"
      ],
      "metadata": {
        "id": "axScBSiclc_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_binary_model(X_train_binary, y_train_binary):\n",
        "    model = Sequential([\n",
        "        LSTM(100, input_shape=(187, 1), return_sequences=False),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_binary, y_train_binary,\n",
        "        validation_split=0.15,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "wlu_U8CemOOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_anomaly_model(X_train_anomaly, y_train_anomaly):\n",
        "    model = Sequential([\n",
        "        LSTM(100, input_shape=(187, 1), return_sequences=False),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(4, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        # Ajouter :\n",
        "        loss_weights=None,\n",
        "        weighted_metrics=None\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.2,\n",
        "        patience=3,\n",
        "        min_lr=0.00001,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_anomaly, y_train_anomaly,\n",
        "        validation_split=0.15,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "s1Cv8Zwqmz2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM avec attention"
      ],
      "metadata": {
        "id": "qYbvsUybm2Wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_binary_model():\n",
        "    \"\"\"Crée le modèle binaire avec attention et régularisation\"\"\"\n",
        "    inputs = Input(shape=(187, 1))\n",
        "    lstm_out = LSTM(64, return_sequences=True,\n",
        "                   kernel_regularizer=l2(0.01))(inputs)\n",
        "\n",
        "    attention = Dense(1, activation='tanh')(lstm_out)\n",
        "    attention_weights = Activation('softmax')(attention)\n",
        "    context = Multiply()([lstm_out, attention_weights])\n",
        "    context = Lambda(lambda x: K.sum(x, axis=1))(context)\n",
        "\n",
        "    dense = Dense(32, activation='relu')(context)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "ND1mQHn8m2Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_anomaly_model():\n",
        "    \"\"\"Crée le modèle d'anomalies avec attention et régularisation\"\"\"\n",
        "    inputs = Input(shape=(187, 1))\n",
        "    lstm_out = LSTM(64, return_sequences=True,\n",
        "                   kernel_regularizer=l2(0.01))(inputs)\n",
        "\n",
        "    attention = Dense(1, activation='tanh')(lstm_out)\n",
        "    attention_weights = Activation('softmax')(attention)\n",
        "    context = Multiply()([lstm_out, attention_weights])\n",
        "    context = Lambda(lambda x: K.sum(x, axis=1))(context)\n",
        "\n",
        "    dense = Dense(32, activation='relu')(context)\n",
        "    outputs = Dense(4, activation='softmax')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Olh5TJn8nGTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction du GRU"
      ],
      "metadata": {
        "id": "Vn7j0IAZnIvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_binary_model():\n",
        "    \"\"\"Version simplifiée sans attention\"\"\"\n",
        "    inputs = Input(shape=(187, 1))\n",
        "\n",
        "    # GRU simple avec dropout\n",
        "    x = GRU(64, return_sequences=False,\n",
        "            kernel_regularizer=l2(0.01),\n",
        "            recurrent_dropout=0.2)(inputs)\n",
        "\n",
        "    # Couches denses avec dropout\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_anomaly_model():\n",
        "    \"\"\"Version simplifiée sans attention\"\"\"\n",
        "    inputs = Input(shape=(187, 1))\n",
        "\n",
        "    # GRU simple avec dropout\n",
        "    x = GRU(64, return_sequences=False,\n",
        "            kernel_regularizer=l2(0.01),\n",
        "            recurrent_dropout=0.2)(inputs)\n",
        "\n",
        "    # Couches denses avec dropout\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "YgZ922FInJIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRU avec attention"
      ],
      "metadata": {
        "id": "02ZK5urhndPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_binary_model():\n",
        "    \"\"\"Modèle binaire avec GRU et Transformer\"\"\"\n",
        "    inputs = Input(shape=(187, 1))\n",
        "\n",
        "    # GRU Layer\n",
        "    gru_out = GRU(64, return_sequences=True,\n",
        "                  kernel_regularizer=l2(0.01))(inputs)\n",
        "\n",
        "    # Transformer Encoder Layer\n",
        "    attention_out = MultiHeadAttention(num_heads=4, key_dim=64)(gru_out, gru_out)\n",
        "    attention_out = Add()([gru_out, attention_out])\n",
        "    attention_out = LayerNormalization()(attention_out)\n",
        "\n",
        "    # FFN\n",
        "    ffn_out = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(attention_out)\n",
        "    ffn_out = Dropout(0.3)(ffn_out)\n",
        "    ffn_out = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(ffn_out)\n",
        "    ffn_out = Add()([attention_out, ffn_out])\n",
        "    ffn_out = LayerNormalization()(ffn_out)\n",
        "\n",
        "    # Pooling\n",
        "    global_pooling = Lambda(lambda x: tf.reduce_mean(x, axis=1))(ffn_out)\n",
        "\n",
        "    # Dense layers\n",
        "    dense = Dense(32, activation='relu', kernel_regularizer=l2(0.005))(global_pooling)\n",
        "    dense = Dropout(0.3)(dense)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_anomaly_model():\n",
        "    inputs = Input(shape=(187, 1))\n",
        "\n",
        "    # GRU avec optimisations\n",
        "    gru_out = GRU(64, return_sequences=True,\n",
        "                  kernel_regularizer=l2(0.01),\n",
        "                  implementation=2,  # Implémentation plus rapide\n",
        "                  reset_after=True)(inputs)\n",
        "\n",
        "    # Réduire la complexité du réseau\n",
        "    dense = Dense(32, activation='relu')(gru_out)\n",
        "    outputs = Dense(4, activation='softmax')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "KymbOBj2ndBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimisation du GRU et transformers"
      ],
      "metadata": {
        "id": "3lcJWKFUnnfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import mixed_precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ],
      "metadata": {
        "id": "gyEJ6PnvnnGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline optimisé pour les données\n",
        "def prepare_tf_data(X, y, batch_size=128):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "zxKuxa_Fn0xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des datasets optimisés\n",
        "train_dataset = prepare_tf_data(X_train, y_train)\n",
        "val_dataset = prepare_tf_data(X_test, y_test)"
      ],
      "metadata": {
        "id": "9GeQ8x7Dn1Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modèle Transformers"
      ],
      "metadata": {
        "id": "xkAAdtzxn66a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_binary_model():\n",
        "    \"\"\"Modèle binaire avec Transformer\"\"\"\n",
        "    inputs = Input(shape=(187, 1))\n",
        "\n",
        "    # Projection initiale\n",
        "    x = Dense(32)(inputs)\n",
        "\n",
        "    # Transformer avec une seule couche d'attention\n",
        "    attention = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "    attention = Add()([x, attention])\n",
        "    attention = LayerNormalization()(attention)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn = Dense(64, activation='relu')(attention)\n",
        "    ffn = Dropout(0.2)(ffn)\n",
        "    ffn = Dense(32)(ffn)\n",
        "    ffn = Add()([attention, ffn])\n",
        "    ffn = LayerNormalization()(ffn)\n",
        "\n",
        "    # Pooling et classification\n",
        "    x = tf.reduce_mean(ffn, axis=1)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_anomaly_model():\n",
        "    \"\"\"Modèle anomalies avec Transformer\"\"\"\n",
        "    inputs = Input(shape=(187, 1))\n",
        "\n",
        "    # Projection initiale\n",
        "    x = Dense(32)(inputs)\n",
        "\n",
        "    # Premier bloc Transformer\n",
        "    attention = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
        "    attention = Add()([x, attention])\n",
        "    x = LayerNormalization()(attention)\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ffn = Dense(64, activation='relu')(x)\n",
        "    ffn = Dropout(0.2)(ffn)\n",
        "    ffn = Dense(32)(ffn)\n",
        "    x = Add()([x, ffn])\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Pooling et classification\n",
        "    x = tf.reduce_mean(x, axis=1)\n",
        "    x = Dense(32, activation='relu')(x)\n",
        "    outputs = Dense(4, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "UMbvy-DMn7Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modèle transformers et gru"
      ],
      "metadata": {
        "id": "i8tQSFexomM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Bloc Transformer Encoder avec attention multi-têtes et connexion résiduelle.\n",
        "    \"\"\"\n",
        "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "    x = Add()([attention, inputs])  # Connexion résiduelle\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = Dropout(dropout)(ff)\n",
        "    ff = Dense(inputs.shape[-1])(ff)\n",
        "    x = Add()([ff, x])  # Connexion résiduelle\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def attention_layer(inputs):\n",
        "    \"\"\"\n",
        "    Mécanisme d'attention classique.\n",
        "    \"\"\"\n",
        "    attention_weights = Dense(1, activation=\"tanh\")(inputs)\n",
        "    attention_weights = Dense(1, activation=\"softmax\", name=\"attention_weights\")(attention_weights)\n",
        "    context_vector = Lambda(lambda x: K.sum(x, axis=1))(inputs * attention_weights)\n",
        "    return context_vector\n",
        "\n",
        "def create_combined_model(input_shape, num_classes, head_size=64, num_heads=4, ff_dim=128, num_transformer_blocks=2, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Modèle combiné Transformers et Attention.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Blocs Transformers\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    # Mécanisme d'attention\n",
        "    attention_output = attention_layer(x)\n",
        "\n",
        "    # Dense Layers\n",
        "    dense = Dense(64, activation=\"relu\")(attention_output)\n",
        "    dense = Dropout(0.2)(dense)\n",
        "    outputs = Dense(num_classes, activation=\"softmax\" if num_classes > 1 else \"sigmoid\")(dense)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=\"sparse_categorical_crossentropy\" if num_classes > 1 else \"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "tF55G105rh9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Utile"
      ],
      "metadata": {
        "id": "3CLqce6lom31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retirer les 0 et normaliser le tous"
      ],
      "metadata": {
        "id": "PX435JM5oym4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_trailing_zeros_from_signal(signal):\n",
        "    \"\"\"Retire les zéros à la fin d'un signal ECG\"\"\"\n",
        "    signal = np.array(signal)\n",
        "    last_nonzero = -1\n",
        "    for i in reversed(range(len(signal))):\n",
        "        if abs(signal[i]) > 1e-10:\n",
        "            last_nonzero = i\n",
        "            break\n",
        "    return signal[:last_nonzero + 1]\n",
        "\n",
        "def normalize_signal_length(signal, target_length=187):\n",
        "    current_length = len(signal)\n",
        "    if current_length < target_length:\n",
        "        # Utiliser l'interpolation au lieu du padding avec des zéros\n",
        "        x_current = np.linspace(0, 1, current_length)\n",
        "        x_target = np.linspace(0, 1, target_length)\n",
        "        return np.interp(x_target, x_current, signal)\n",
        "    return signal[:target_length]"
      ],
      "metadata": {
        "id": "aD9G-9wHo2mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code pour faire la séparation, pour les deux modèles"
      ],
      "metadata": {
        "id": "qNVDnghGo3N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_binary_data(df):\n",
        "    \"\"\"Génère les données pour classification binaire\"\"\"\n",
        "    ecgs, labels = [], []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        # Nettoyage du signal\n",
        "        signal = row.iloc[0:187].values\n",
        "        cleaned_signal = trim_trailing_zeros_from_signal(signal)\n",
        "        normalized_signal = normalize_signal_length(cleaned_signal)\n",
        "\n",
        "        ecgs.append(normalized_signal)\n",
        "        labels.append(1 if int(row.iloc[187]) != 0 else 0)\n",
        "    return np.array(ecgs), np.array(labels)\n",
        "\n",
        "def prepare_anomaly_data(train_df, test_df):\n",
        "    \"\"\"Prépare les données pour le modèle d'anomalies\"\"\"\n",
        "    X_train_anomaly, y_train_anomaly = [], []\n",
        "    X_test_anomaly, y_test_anomaly = [], []\n",
        "\n",
        "    # Traitement des données d'entraînement\n",
        "    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "        if int(row.iloc[187]) != 0:\n",
        "            # Nettoyage du signal\n",
        "            signal = row.iloc[0:187].values\n",
        "            cleaned_signal = trim_trailing_zeros_from_signal(signal)\n",
        "            normalized_signal = normalize_signal_length(cleaned_signal)\n",
        "\n",
        "            X_train_anomaly.append(normalized_signal)\n",
        "            label = int(row.iloc[187]) - 1 if int(row.iloc[187]) != 4 else 3\n",
        "            y_train_anomaly.append(label)\n",
        "\n",
        "    # Traitement des données de test\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "        if int(row.iloc[187]) != 0:\n",
        "            signal = row.iloc[0:187].values\n",
        "            cleaned_signal = trim_trailing_zeros_from_signal(signal)\n",
        "            normalized_signal = normalize_signal_length(cleaned_signal)\n",
        "\n",
        "            X_test_anomaly.append(normalized_signal)\n",
        "            label = int(row.iloc[187]) - 1 if int(row.iloc[187]) != 4 else 3\n",
        "            y_test_anomaly.append(label)\n",
        "\n",
        "    # Conversion et normalisation\n",
        "    X_train_anomaly = np.array(X_train_anomaly)\n",
        "    X_test_anomaly = np.array(X_test_anomaly)\n",
        "    y_train_anomaly = np.array(y_train_anomaly)\n",
        "    y_test_anomaly = np.array(y_test_anomaly)\n",
        "\n",
        "    # Normalisation\n",
        "    scaler = StandardScaler()\n",
        "    X_train_anomaly = scaler.fit_transform(X_train_anomaly)\n",
        "    X_test_anomaly = scaler.transform(X_test_anomaly)\n",
        "\n",
        "    # Reshape pour LSTM\n",
        "    X_train_anomaly = X_train_anomaly.reshape(X_train_anomaly.shape[0], X_train_anomaly.shape[1], 1)\n",
        "    X_test_anomaly = X_test_anomaly.reshape(X_test_anomaly.shape[0], X_test_anomaly.shape[1], 1)\n",
        "\n",
        "    return X_train_anomaly, X_test_anomaly, y_train_anomaly, y_test_anomaly"
      ],
      "metadata": {
        "id": "c7WO_4xzo3mU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification que l'augmentation fonctionne"
      ],
      "metadata": {
        "id": "wo8aFblipCfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_augmentation():\n",
        "    \"\"\"Vérifie la data augmentation et sa distribution\"\"\"\n",
        "    # Afficher la distribution avant augmentation\n",
        "    print(\"=== Distribution avant augmentation ===\")\n",
        "    unique, counts = np.unique(y_train_anomaly, return_counts=True)\n",
        "    for u, c in zip(unique, counts):\n",
        "        print(f\"Classe {u}: {c} échantillons\")\n",
        "\n",
        "    # Afficher la distribution après augmentation\n",
        "    print(\"\\n=== Distribution après augmentation ===\")\n",
        "    unique, counts = np.unique(y_train_anomaly_balanced, return_counts=True)\n",
        "    for u, c in zip(unique, counts):\n",
        "        print(f\"Classe {u}: {c} échantillons\")\n",
        "\n",
        "    # Visualiser un exemple de signal original et augmenté pour la classe 2\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Signal original de la classe 2\n",
        "    idx_original = np.where(y_train_anomaly == 2)[0][0]\n",
        "    plt.subplot(121)\n",
        "    plt.plot(X_train_anomaly[idx_original])\n",
        "    plt.title(\"Signal original - Classe 2\")\n",
        "\n",
        "    # Signal augmenté de la classe 2\n",
        "    idx_augmented = np.where(y_train_anomaly_balanced == 2)[0][-1]\n",
        "    plt.subplot(122)\n",
        "    plt.plot(X_train_anomaly_balanced[idx_augmented])\n",
        "    plt.title(\"Signal augmenté - Classe 2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IEyiQSYFpCvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification que les 0 des signaux ont bien été supprimé"
      ],
      "metadata": {
        "id": "exksAKTupUgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_cleaning():\n",
        "    \"\"\"Vérifie le nettoyage des signaux\"\"\"\n",
        "    train_df = pd.read_csv(train_path)\n",
        "\n",
        "    # Exemple avec un signal\n",
        "    original_signal = train_df.iloc[0, 0:187].values\n",
        "    cleaned_signal = trim_trailing_zeros_from_signal(original_signal)\n",
        "    normalized_signal = normalize_signal_length(cleaned_signal)\n",
        "\n",
        "    # Visualisation\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(131)\n",
        "    plt.plot(original_signal)\n",
        "    plt.title(\"Signal original\")\n",
        "\n",
        "    plt.subplot(132)\n",
        "    plt.plot(cleaned_signal)\n",
        "    plt.title(\"Signal sans zéros trailing\")\n",
        "\n",
        "    plt.subplot(133)\n",
        "    plt.plot(normalized_signal)\n",
        "    plt.title(\"Signal normalisé\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Longueur originale: {len(original_signal)}\")\n",
        "    print(f\"Longueur après nettoyage: {len(cleaned_signal)}\")\n",
        "    print(f\"Longueur après normalisation: {len(normalized_signal)}\")"
      ],
      "metadata": {
        "id": "0vn5BHslpUvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voir la distribution des classes"
      ],
      "metadata": {
        "id": "AEtCG5Z_p3O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_distribution(original_labels, augmented_labels, label_names=None):\n",
        "    \"\"\"\n",
        "    Compare la distribution des classes avant et après augmentation.\n",
        "\n",
        "    :param original_labels: Labels originaux (avant augmentation)\n",
        "    :param augmented_labels: Labels après augmentation\n",
        "    :param label_names: Liste des noms des classes (optionnel)\n",
        "    \"\"\"\n",
        "    original_counts = np.bincount(original_labels)\n",
        "    augmented_counts = np.bincount(augmented_labels)\n",
        "\n",
        "    x = np.arange(len(original_counts))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Distribution originale\n",
        "    plt.bar(x - 0.2, original_counts, width=0.4, label='Original', color='blue', alpha=0.7)\n",
        "\n",
        "    # Distribution après augmentation\n",
        "    plt.bar(x + 0.2, augmented_counts, width=0.4, label='Augmented', color='orange', alpha=0.7)\n",
        "\n",
        "    # Ajout des labels\n",
        "    if label_names:\n",
        "        plt.xticks(x, label_names, rotation=45)\n",
        "    else:\n",
        "        plt.xticks(x, [f'Classe {i}' for i in x])\n",
        "\n",
        "    plt.title(\"Distribution des classes avant et après augmentation\")\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Nombre d'échantillons\")\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "c9KC0R7Up2kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_anomaly_class_distribution(original_labels, augmented_labels, label_names=None):\n",
        "    \"\"\"\n",
        "    Compare la distribution des classes d'anomalies (malades) avant et après augmentation.\n",
        "\n",
        "    :param original_labels: Labels originaux (avant augmentation)\n",
        "    :param augmented_labels: Labels après augmentation\n",
        "    :param label_names: Liste des noms des classes (optionnel)\n",
        "    \"\"\"\n",
        "    original_counts = np.bincount(original_labels)\n",
        "    augmented_counts = np.bincount(augmented_labels)\n",
        "\n",
        "    x = np.arange(len(original_counts))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Distribution originale\n",
        "    plt.bar(x - 0.2, original_counts, width=0.4, label='Original', color='blue', alpha=0.7)\n",
        "\n",
        "    # Distribution après augmentation\n",
        "    plt.bar(x + 0.2, augmented_counts, width=0.4, label='Augmented', color='orange', alpha=0.7)\n",
        "\n",
        "    # Ajout des labels\n",
        "    if label_names:\n",
        "        plt.xticks(x, label_names, rotation=45)\n",
        "    else:\n",
        "        plt.xticks(x, [f'Classe {i+1}' for i in x])  # Les classes des anomalies\n",
        "\n",
        "    plt.title(\"Distribution des classes d'anomalies avant et après augmentation\")\n",
        "    plt.xlabel(\"Classes d'anomalies\")\n",
        "    plt.ylabel(\"Nombre d'échantillons\")\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-qPD6OEHpq6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Évaluer le modèle"
      ],
      "metadata": {
        "id": "N_uW6PKsqGZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, is_binary=True):\n",
        "    \"\"\"Évalue les performances du modèle\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    if is_binary:\n",
        "        y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "    else:\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    print(\"\\nRapport de classification:\")\n",
        "    print(classification_report(y_test, y_pred_classes))\n",
        "\n",
        "    # Matrice de confusion\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = confusion_matrix(y_test, y_pred_classes)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Matrice de confusion')\n",
        "    plt.ylabel('Vraie classe')\n",
        "    plt.xlabel('Classe prédite')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7JXqubV4qIID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC"
      ],
      "metadata": {
        "id": "WRyRpPyzqHwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction pour tracer la courbe ROC\n",
        "def plot_roc_curve(y_test, y_pred_proba):\n",
        "    n_classes = 5\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve((y_test == i).astype(int), y_pred_proba[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(n_classes):\n",
        "        plt.plot(fpr[i], tpr[i], label=f'Classe {i} (AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('Taux de faux positifs')\n",
        "    plt.ylabel('Taux de vrais positifs')\n",
        "    plt.title('Courbe ROC')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KiOekL1oqJ6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voir les résultats détaillés\n",
        "\n"
      ],
      "metadata": {
        "id": "lUyWzSXSqqPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Métriques détaillées sur le jeu de test\n",
        "label_mapping = {\n",
        "    0: \"Normal\",\n",
        "    1: \"Artial Premature\",\n",
        "    2: \"Premature ventricular contraction\",\n",
        "    3: \"Fusion of ventricular and normal\",\n",
        "    4: \"Fusion of paced and normal\"\n",
        "}\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "roc_scores = {}\n",
        "for i in range(5):\n",
        "    roc_scores[i] = roc_auc_score((y_test == i).astype(int), y_pred[:, i])\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"\\nRésultats sur le jeu de test:\")\n",
        "print(\"\\nMatrice de Confusion:\")\n",
        "print(pd.DataFrame(\n",
        "    cm,\n",
        "    index=[label_mapping[i] for i in range(5)],\n",
        "    columns=[label_mapping[i] for i in range(5)]\n",
        "))\n",
        "\n",
        "print(\"\\nScores ROC AUC par classe:\")\n",
        "print(pd.DataFrame({\n",
        "    'Classe': [label_mapping[i] for i in range(5)],\n",
        "    'Score ROC AUC': [roc_scores[i] for i in range(5)]\n",
        "}))\n",
        "\n",
        "print(\"\\nRapport de classification détaillé:\")\n",
        "print(classification_report(y_test, y_pred_classes,\n",
        "                          target_names=[label_mapping[i] for i in range(5)]))"
      ],
      "metadata": {
        "id": "ogAuAqJYqqeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code final implémentant toutes les fonctionnalités"
      ],
      "metadata": {
        "id": "OOhyn2aArvOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importations nécessaires\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, Input, Lambda, LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Dense, Dropout, LayerNormalization, Add, Input, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Configuration GPU et mixed precision\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DbV94vDtrxa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonctions pour préparer et nettoyer les données\n",
        "def trim_trailing_zeros_from_signal(signal):\n",
        "    \"\"\"Retire les zéros à la fin d'un signal ECG.\"\"\"\n",
        "    signal = np.array(signal)\n",
        "    last_nonzero = -1\n",
        "    for i in reversed(range(len(signal))):\n",
        "        if abs(signal[i]) > 1e-10:\n",
        "            last_nonzero = i\n",
        "            break\n",
        "    return signal[:last_nonzero + 1]\n",
        "\n",
        "def normalize_signal_length(signal, target_length=187):\n",
        "    \"\"\"Normalise la longueur d'un signal.\"\"\"\n",
        "    current_length = len(signal)\n",
        "    if current_length < target_length:\n",
        "        x_current = np.linspace(0, 1, current_length)\n",
        "        x_target = np.linspace(0, 1, target_length)\n",
        "        return np.interp(x_target, x_current, signal)\n",
        "    return signal[:target_length]\n",
        "\n",
        "def prepare_data(train_path, test_path):\n",
        "    \"\"\"Charge et prépare les données d'entraînement et de test.\"\"\"\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    X_train, y_train = generate_binary_data(train_df)\n",
        "    X_test, y_test = generate_binary_data(test_df)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def generate_binary_data(df):\n",
        "    \"\"\"Génère les données pour classification binaire.\"\"\"\n",
        "    ecgs, labels = [], []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        signal = row.iloc[0:187].values\n",
        "        cleaned_signal = trim_trailing_zeros_from_signal(signal)\n",
        "        normalized_signal = normalize_signal_length(cleaned_signal)\n",
        "        ecgs.append(normalized_signal)\n",
        "        labels.append(1 if int(row.iloc[187]) != 0 else 0)\n",
        "    return np.array(ecgs), np.array(labels)\n",
        "\n",
        "def prepare_anomaly_data(train_df, test_df):\n",
        "    \"\"\"Prépare les données pour le modèle d'anomalies\"\"\"\n",
        "    X_train_anomaly, y_train_anomaly = [], []\n",
        "    X_test_anomaly, y_test_anomaly = [], []\n",
        "\n",
        "    # Traitement des données d'entraînement\n",
        "    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "        if int(row.iloc[187]) != 0:\n",
        "            # Nettoyage du signal\n",
        "            signal = row.iloc[0:187].values\n",
        "            cleaned_signal = trim_trailing_zeros_from_signal(signal)\n",
        "            normalized_signal = normalize_signal_length(cleaned_signal)\n",
        "\n",
        "            X_train_anomaly.append(normalized_signal)\n",
        "            label = int(row.iloc[187]) - 1 if int(row.iloc[187]) != 4 else 3\n",
        "            y_train_anomaly.append(label)\n",
        "\n",
        "    # Traitement des données de test\n",
        "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "        if int(row.iloc[187]) != 0:\n",
        "            signal = row.iloc[0:187].values\n",
        "            cleaned_signal = trim_trailing_zeros_from_signal(signal)\n",
        "            normalized_signal = normalize_signal_length(cleaned_signal)\n",
        "\n",
        "            X_test_anomaly.append(normalized_signal)\n",
        "            label = int(row.iloc[187]) - 1 if int(row.iloc[187]) != 4 else 3\n",
        "            y_test_anomaly.append(label)\n",
        "\n",
        "    # Conversion et normalisation\n",
        "    X_train_anomaly = np.array(X_train_anomaly)\n",
        "    X_test_anomaly = np.array(X_test_anomaly)\n",
        "    y_train_anomaly = np.array(y_train_anomaly)\n",
        "    y_test_anomaly = np.array(y_test_anomaly)\n",
        "\n",
        "    # Normalisation\n",
        "    scaler = StandardScaler()\n",
        "    X_train_anomaly = scaler.fit_transform(X_train_anomaly)\n",
        "    X_test_anomaly = scaler.transform(X_test_anomaly)\n",
        "\n",
        "    # Reshape pour LSTM\n",
        "    X_train_anomaly = X_train_anomaly.reshape(X_train_anomaly.shape[0], X_train_anomaly.shape[1], 1)\n",
        "    X_test_anomaly = X_test_anomaly.reshape(X_test_anomaly.shape[0], X_test_anomaly.shape[1], 1)\n",
        "\n",
        "    return X_train_anomaly, X_test_anomaly, y_train_anomaly, y_test_anomaly"
      ],
      "metadata": {
        "id": "aWIXSQsFsw0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size=64, num_heads=4, ff_dim=128, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Implémente un encodeur Transformer.\n",
        "\n",
        "    Args:\n",
        "        inputs: Entrée du modèle.\n",
        "        head_size: Dimension des têtes d'attention.\n",
        "        num_heads: Nombre de têtes d'attention.\n",
        "        ff_dim: Dimension du réseau feed-forward.\n",
        "        dropout: Taux de dropout.\n",
        "\n",
        "    Returns:\n",
        "        La sortie de l'encodeur Transformer.\n",
        "    \"\"\"\n",
        "    # Attention multi-têtes\n",
        "    attention = MultiHeadAttention(num_heads=num_heads, key_dim=head_size)(inputs, inputs)\n",
        "    attention = Dropout(dropout)(attention)\n",
        "    x = Add()([attention, inputs])  # Connexion résiduelle\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # Réseau feed-forward\n",
        "    ff = Dense(ff_dim, activation=\"relu\")(x)\n",
        "    ff = Dropout(dropout)(ff)\n",
        "    ff = Dense(inputs.shape[-1])(ff)\n",
        "    x = Add()([ff, x])  # Connexion résiduelle\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def create_transformer_model(input_shape, num_classes, head_size=64, num_heads=2, ff_dim=128, num_transformer_blocks=2, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Crée un modèle Transformers pour la classification.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Forme des données d'entrée.\n",
        "        num_classes: Nombre de classes en sortie.\n",
        "        head_size: Dimension des têtes d'attention.\n",
        "        num_heads: Nombre de têtes d'attention.\n",
        "        ff_dim: Dimension du réseau feed-forward.\n",
        "        num_transformer_blocks: Nombre de blocs Transformers.\n",
        "        dropout: Taux de dropout.\n",
        "\n",
        "    Returns:\n",
        "        Modèle Keras.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    # Ajouter plusieurs blocs Transformers\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    # Pooling global (moyenne des features)\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    # Couche dense finale\n",
        "    outputs = Dense(num_classes, activation=\"softmax\" if num_classes > 1 else \"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=\"sparse_categorical_crossentropy\" if num_classes > 1 else \"binary_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "iW5yBGPZs0_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(X_data, y_data, target_class, augmentation_factor):\n",
        "    \"\"\"Augmentation améliorée des données\"\"\"\n",
        "    class_indices = np.where(y_data == target_class)[0]\n",
        "    X_class = X_data[class_indices]\n",
        "    augmented_X = []\n",
        "    augmented_y = []\n",
        "\n",
        "    for signal in X_class:\n",
        "        for _ in range(augmentation_factor):\n",
        "            # Combinaison de plusieurs techniques\n",
        "            augmented_signal = signal.copy()\n",
        "\n",
        "            # Bruit gaussien avec amplitude variable\n",
        "            noise_level = np.random.uniform(0.01, 0.03)\n",
        "            augmented_signal += np.random.normal(0, noise_level, signal.shape)\n",
        "\n",
        "            # Décalage temporel aléatoire\n",
        "            shift = np.random.randint(-10, 10)\n",
        "            augmented_signal = np.roll(augmented_signal, shift, axis=0)\n",
        "\n",
        "            # Scaling amplitude\n",
        "            scale = np.random.uniform(0.9, 1.1)\n",
        "            augmented_signal *= scale\n",
        "\n",
        "            # Inversion temporelle aléatoire\n",
        "            if np.random.random() > 0.5:\n",
        "                augmented_signal = augmented_signal[::-1]\n",
        "\n",
        "            augmented_X.append(augmented_signal)\n",
        "            augmented_y.append(target_class)\n",
        "\n",
        "    return np.array(augmented_X), np.array(augmented_y)\n",
        "\n",
        "def balance_classes(X_data, y_data):\n",
        "    \"\"\"Version améliorée de l'équilibrage des classes\"\"\"\n",
        "    class_counts = np.bincount(y_data)\n",
        "    target_count = np.max(class_counts)\n",
        "\n",
        "    # Stockage des données augmentées\n",
        "    all_augmented_X = []\n",
        "    all_augmented_y = []\n",
        "\n",
        "    for class_label in range(len(class_counts)):\n",
        "        if class_counts[class_label] < target_count:\n",
        "            needed = target_count - class_counts[class_label]\n",
        "\n",
        "            # Calculer le facteur d'augmentation\n",
        "            base_count = class_counts[class_label]\n",
        "            augmentation_factor = int(np.ceil(needed / base_count))\n",
        "\n",
        "            # Générer plus de données que nécessaire\n",
        "            X_aug, y_aug = augment_data(\n",
        "                X_data, y_data,\n",
        "                class_label, augmentation_factor\n",
        "            )\n",
        "\n",
        "            # Sélectionner le nombre exact nécessaire\n",
        "            indices = np.random.choice(len(X_aug), needed, replace=False)\n",
        "            all_augmented_X.append(X_aug[indices])\n",
        "            all_augmented_y.append(y_aug[indices])\n",
        "\n",
        "    if len(all_augmented_X) > 0:\n",
        "        X_balanced = np.concatenate([X_data] + all_augmented_X)\n",
        "        y_balanced = np.concatenate([y_data] + all_augmented_y)\n",
        "\n",
        "        # Mélanger les données\n",
        "        indices = np.random.permutation(len(X_balanced))\n",
        "        X_balanced = X_balanced[indices]\n",
        "        y_balanced = y_balanced[indices]\n",
        "\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "    return X_data, y_data"
      ],
      "metadata": {
        "id": "hex-44wEtLrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline optimisé pour les données\n",
        "def prepare_tf_data(X, y, batch_size=128):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "MvbfT9SjtFeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction d'entraînement avec callbacks\n",
        "def train_model(model, train_dataset, val_dataset, epochs=20):\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        validation_data=val_dataset,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "    return history"
      ],
      "metadata": {
        "id": "911_cVSYtSjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Évaluation et rapport des résultats\n",
        "def evaluate_model(model, test_dataset, is_binary=True):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    for X_batch, y_batch in test_dataset:\n",
        "        y_pred.extend(model.predict(X_batch))\n",
        "        y_true.extend(y_batch.numpy())\n",
        "\n",
        "    y_pred_classes = (np.array(y_pred) > 0.5).astype(int) if is_binary else np.argmax(y_pred, axis=1)\n",
        "    print(\"\\nRapport de classification:\")\n",
        "    print(classification_report(y_true, y_pred_classes))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_classes)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Matrice de confusion')\n",
        "    plt.ylabel('Vraie classe')\n",
        "    plt.xlabel('Classe prédite')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "G-DrMRs7tUxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle Transformers pour la classification binaire\n",
        "transformer_binary_model = create_transformer_model(\n",
        "    input_shape=(187, 1),\n",
        "    num_classes=1,\n",
        "    head_size=32,\n",
        "    num_heads=2,\n",
        "    ff_dim=64,\n",
        "    num_transformer_blocks=2,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Entraînement du modèle binaire\n",
        "binary_transformer_history = train_model(\n",
        "    transformer_binary_model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "# Évaluation du modèle Transformers (binaire)\n",
        "print(\"\\nÉvaluation du modèle Transformers (binaire) :\")\n",
        "evaluate_model(transformer_binary_model, val_dataset, is_binary=True)"
      ],
      "metadata": {
        "id": "sXyM00EetVaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle Transformers pour les anomalies\n",
        "transformer_anomaly_model = create_transformer_model(\n",
        "    input_shape=(187, 1),\n",
        "    num_classes=4,\n",
        "    head_size=32,\n",
        "    num_heads=2,\n",
        "    ff_dim=64,\n",
        "    num_transformer_blocks=2,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "# Entraînement du modèle pour les anomalies\n",
        "anomaly_transformer_history = train_model(\n",
        "    transformer_anomaly_model,\n",
        "    anomaly_dataset,\n",
        "    val_anomaly_dataset,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "# Évaluation du modèle Transformers (anomalies)\n",
        "print(\"\\nÉvaluation du modèle Transformers (anomalies) :\")\n",
        "evaluate_model(transformer_anomaly_model, val_anomaly_dataset, is_binary=False)\n"
      ],
      "metadata": {
        "id": "kc1XZlUsty3p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}